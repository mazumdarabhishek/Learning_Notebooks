{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Pre-training \n",
    "BERT for pre-training have two output types NSP- Next Sentence Prediction & MLM - Masked Language Modelling heads. We use these two heads to fine tune our model and once thats done, we can get rid of the two heads and use Bert as is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForPreTraining\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d0dc954e1840c291ecfcdeabfe8049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffe1adfeddc4aa2af254760d86eb82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc86f097a4d34e428f892bbd9e4e3d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForPreTraining.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"After Abraham Lincoln won the November 1860 presidential [MASK] on an \"\n",
    "        \"anti-slavery platform, an initial seven slave states declared their \"\n",
    "        \"secession from the country to form the Confederacy. War broke out in \"\n",
    "        \"April 1861 when secessionist forces [MASK] Fort Sumter in South \"\n",
    "        \"Carolina, just over a month after Lincoln's inauguration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['prediction_logits', 'seq_relationship_logits'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 62, 30522]), torch.Size([1, 2]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['prediction_logits'].shape, outputs['seq_relationship_logits'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **prediction_logits** - They are of shape [1, 62, 30522]. This means that these are the MLM head output as 62 denotes the number of tokens and 30522 is the length of each token vector as the vocab size of bert is little over 30,000\n",
    "- **seq_relationship_logits** - These are the outputs from the NSP head which contains the logits scores for whether a given sentence is or not the next sentence of a given previous sentence. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7976"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "vocab[\"artificial\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reverse the vocab to id:value pairs\n",
    "idx_vocab = {value:key for key, value in vocab.items()}\n",
    "idx_vocab[7976]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30522]), 30522)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check the size similarity between vocab and MLM output\n",
    "outputs['prediction_logits'][0][2].shape, len(idx_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 - SOFTMAX to convert the logits into probability scores\n",
    "softmax = torch.nn.functional.softmax(input = outputs['prediction_logits'][0][2], dim=-1)\n",
    "\n",
    "# STEP 2 - ARGMAX to get the token id with maximum probability\n",
    "argmax = torch.argmax(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8181)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abraham'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_vocab[argmax.item()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For given input text with MASK token:\n",
    "\n",
    "\"After Abraham Lincoln won the November 1860 presidential [MASK] on an \"\n",
    "\"anti-slavery platform, an initial seven slave states declared their \"\n",
    "\"secession from the country to form the Confederacy. War broke out in \"\n",
    "\"April 1861 when secessionist forces [MASK] Fort Sumter in South \"\n",
    "\"Carolina, just over a month after Lincoln's inauguration.\"\n",
    "\n",
    "The value for 2nd index was predicted as 'abraham'\n",
    "\n",
    "For the rest of the words that were not masked. they will return the id for default input words(close match not exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.6535e-04, 9.2138e-04, 9.4259e-04,  ..., 1.0541e-03, 2.1833e-03,\n",
       "         5.1356e-03],\n",
       "        [6.2549e-06, 7.3302e-06, 6.1164e-06,  ..., 6.1201e-06, 1.9717e-05,\n",
       "         5.6888e-05],\n",
       "        [3.4552e-03, 3.0115e-03, 5.1767e-03,  ..., 1.9010e-03, 3.4777e-03,\n",
       "         3.3927e-03],\n",
       "        ...,\n",
       "        [1.8583e-01, 2.0567e-01, 2.1530e-01,  ..., 1.5572e-01, 5.1572e-01,\n",
       "         2.0563e-04],\n",
       "        [1.1240e-06, 1.0609e-06, 1.2606e-06,  ..., 9.1067e-06, 1.4953e-05,\n",
       "         1.7173e-05],\n",
       "        [1.7728e-05, 1.0281e-05, 1.7399e-05,  ..., 1.0896e-05, 1.9261e-04,\n",
       "         5.3675e-05]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "softmax = torch.nn.functional.softmax(input = outputs['prediction_logits'][0], dim=0)\n",
    "softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([28191,  2348,  8181, 16628,  2180,  3882,  2281,  7313,  4883, 27419,\n",
       "         2006,  2010,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
       "         8914,  2163, 13520,  2037,  4336,  2013,  1996,  2406,  2000,  2433,\n",
       "        28775, 18179, 16363,  2162,  3631,  2041,  1999,  2258,  6863,  2043,\n",
       "        18232,  2923,  2749,  4548,  3481,  7680,  5017,  2005,  2148,  3792,\n",
       "        24901,  2074,  2058,  1037,  3204,  2077,  3946,  1005,  1055, 17331,\n",
       "         1025, 25656])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax = torch.argmax(softmax, dim= 1)\n",
    "argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('##ecin although abraham lincolnshire won 1948 november 1860 presidential '\n",
      " 'primaries on his anti - slavery platform , an initial seven tributary states '\n",
      " 'declare their independence from the country to form ##ici confederacy ##yre '\n",
      " 'war broke out in april 1861 when ##oya ##ist forces occupied fort sum ##mer '\n",
      " \"for south carolina ##trip just over a month before grant ' s inauguration ; \"\n",
      " '##tson ')\n"
     ]
    }
   ],
   "source": [
    "predicted_text = \"\"\n",
    "for tk in argmax:\n",
    "    predicted_text += idx_vocab[tk.item()] + \" \"\n",
    "\n",
    "pprint.pprint(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('After Abraham Lincoln won the November 1860 presidential [MASK] on an '\n",
      " 'anti-slavery platform, an initial seven slave states declared their '\n",
      " 'secession from the country to form the Confederacy. War broke out in April '\n",
      " '1861 when secessionist forces [MASK] Fort Sumter in South Carolina, just '\n",
      " \"over a month after Lincoln's inauguration.\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noticed above, the prediction somewhat has a close relationship with the original text. \n",
    "For the Masked token predictions:\n",
    "- 1st Mask is predicted as *primaries* (truth - elections)\n",
    "- 2nd Mask is predicted as *occupied* (truth - attacked)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NSP\n",
    "\n",
    "In order for the NSP to work, we need to input sequence of sentences where it can make NSP predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"After Abraham Lincoln won the November 1860 presidential [MASK] on an \"\n",
    "        \"anti-slavery platform, an initial seven slave states declared their \"\n",
    "        \"secession from the country to form the Confederacy.\")\n",
    "text2 = (\"War broke out in April 1861 when secessionist forces [MASK] Fort \"\n",
    "         \"Sumter in South Carolina, just over a month after Lincoln's \"\n",
    "         \"inauguration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, text2, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2044,  8181,  5367,  2180,  1996,  2281,  7313,  4883,   103,\n",
       "          2006,  2019,  3424,  1011,  8864,  4132,  1010,  2019,  3988,  2698,\n",
       "          6658,  2163,  4161,  2037, 22965,  2013,  1996,  2406,  2000,  2433,\n",
       "          1996, 18179,  1012,   102,  2162,  3631,  2041,  1999,  2258,  6863,\n",
       "          2043, 22965,  2923,  2749,   103,  3481,  7680,  3334,  1999,  2148,\n",
       "          3792,  1010,  2074,  2058,  1037,  3204,  2044,  5367,  1005,  1055,\n",
       "         17331,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- within the tokenizer output dictionary, it is seen that the 'input_ids' have a token 102 (SEP) that separates the first sentence from the second sentence\n",
    "- We have 'token_type_ids' where the token values for first sentence is 0 and that for the second sentence is 1. this is the input that Bert receives which is vital for NSP\n",
    "- we soo no difference in the 'attention_mask'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.0844, -5.6813]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.seq_relationship_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax = torch.argmax(input= outputs.seq_relationship_logits)\n",
    "argmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index 0 - is next sentence\n",
    "\n",
    "Index 1 - in not next sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformerEnv",
   "language": "python",
   "name": "transformerenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
